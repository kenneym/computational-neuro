{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compuational Neuroscience Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:\n",
    "I spent my past summer working in the bioinformatics group of a small biotech company called Cell Signaling Technology. Our company focuses on the development of research antibodies, and much of the software I used involved structural modelling, binding-epitope prediction, binding-affinity prediction, etc. Many of these applications were built by implementing by pre-trained neural nets that could \"map\" antibody residue sequence to binding  site, structural features, etc.\n",
    "\n",
    "## Research Proposal:\n",
    "\n",
    "Although I used these machine-learning-based software packages, I've never done any actual machine learning coding myself. As a first step into machine learning, I attempted to perform a *somewhat* simple task involving antibody sequence processing:\n",
    "\n",
    "- Identify the species of an antibody, based on the nucleotide sequence of its light and heavy chains.\n",
    "\n",
    "It should be noted that there are no existing algorithms, to my knowledge, that attempt to identify the species from which an antibody sequence originated. This is probably because human researchers can do it pretty easily... nevertheless, I think this would be an interesting exercise. As inputs, I am considering looking into the features that Clusal Omega uses to determine sequence alignment, and seeing if using some of the insights from clustal omega could help build a more efficient neural net (to my untrained eye, testing the sequence alignment of two antibodies is the easiest way to see if they're part of the same species... two of the same species will often have ~%70 identity, whereas two of different species will often land around ~30-40% identity).\n",
    "\n",
    "## Objectives of this project\n",
    "\n",
    "- Prior to completing this project, I had never used any of the python libraries you see me using below. Thus, one of the most important objectives here was to begin introducing myself to machine learning through code, and to become familiar with some of the state-of-the-art libraries used to do that. I spent about 20-30 hours taking an udemy course, \"Complete Guide to TensorFlow for Deep Learning with Python\", and learned the basics of the following libraries:\n",
    "    - tensorflow\n",
    "    - pandas\n",
    "    - numpy\n",
    "    - scikitlearn\n",
    "    - matplotlib\n",
    "    \n",
    "- A second objective of this work was to become familiar with how exactly to apply a knowledge of machine learning to code itself- to translate a conceptual understanding into a technical skill. To some extent, I believe that I've made good progress in this area.\n",
    "\n",
    "- Thirdly, I hoped this project would give me a good understanding of how I might apply machine learning to real world data. In this case, I attempted to conjoin a domain-specific knowledge of antibody sequences and structure with a more general understanding of machine learning. Performing this exercise of combining my knowledge on a data set with a knowledge of machine learning will help me to reiterate this process on other forms of data in the time ahead of me. \n",
    "\n",
    "## Project\n",
    "\n",
    "The project itself was originally aimed at identifying the species of origin for an antibody sequence using sequencing data alone. However, because this is a fairly difficult task, and working with data such as images or sequence data requires a more technically complicated recurrent neural net, I've decided to break the project into two pieces.\n",
    "\n",
    "1. Identity the species of origin from which an antibody originated using a simple linear classifier or densely connected neural net, and continuous data points indicating various features about the antibody.\n",
    "\n",
    "2. Attempt to classify the antibody's species of origin using only sequencing information. Specifically, this sequence information will include the amino acid sequence of the antibody's light chain, and the sequences of the it's three complimentary determining regions.\n",
    "\n",
    "## Background\n",
    "\n",
    "Although additionally information about each of the above sub-projects will be provided further down in the code, in order to understand the intent behind the code, a little background on antibodies in general and their place in this project specifically is neccessary. To cover all my ground, I'm going to assume zero prior knowledge on the subject for anyone reviewing this ipython notebook.\n",
    "\n",
    "#### Antibodies Background:\n",
    "\n",
    "Antibodies(Ab), or immunoglobulins (Ig) are immune proteins, generated by nearly countless forms of species, ranging from mammals all the way back to cartilagenous fishes [Schluter et. al, 1977](https://www.ncbi.nlm.nih.gov/pubmed/9386351). In all cases, the primary function of an antibody is to attempt to bind to and neutralize pathogens. When an antibody binds to its target, it can perform one of two things:\n",
    "- Tag the protein, thus signaling the rest of the immune system of a potential intruder\n",
    "- Directly neutralize the target, in the case that the antibody has bound to an area on the pathogen that is biologically relevant and/or necessary for the pathogen's survival\n",
    "\n",
    "Antibodies thus form a critical piece of the body's immune system. The body develops antibodies in mass quantities to target a wide variety of pathogenic or potentially pathogenic proteins and ligands. By doing so, the body is typically able to create a subset of antibodies that can successfully bind to the non-native (or potentially pathogenic) protein that it is attempting to target. \n",
    "\n",
    "Further, once researchers realized the power of antibodies in the human body, they began to utilize antibodies for new tasks. Today, antibodies produced under the supervision of researchers from pharma, biotech, and academic research alike are used for a variety of tasks. Commercially produced antibodies now find therapeutic uses, and are especially useful for patients whose immune systems are not well-equipped to fight an ailment. Keytruda and Optivo, for example, use antibodies to fight a range of advanced cancers. In the case of cancer, the body's immune system does not often recognize cancer cells, causing the disease to spread. Antibody drugs can help to counteract this issue by triggering an immune response. Further, antibodies can be used in therapeutics to directly attack cancers or other pathogens by binding to a functionally critical location and performing neutralization directly. Finally, in addition to all of these therapeutics, antibodies can be used for research... for example, by enabling researchers to tag proteins they are studying in a tissue specimen with a florescent tag or a radioactive tag. These tags can help researchers to visualize and collect data on proteins they are studying.\n",
    "\n",
    "##### Structure\n",
    "\n",
    "To understand how exactly how the antibody has seen such enormous success, however, one has to take a look at its structure, and examine how the body introduces genetic variance. Antibodies express a unique Y-shape, as follows.\n",
    "\n",
    "![Antibody Shape](https://www.immunology.org/sites/default/files/Generation-of-B-cell-antibody-diversity-Figure-1.png)\n",
    "\n",
    "Taking a look at this Y shape, one can identify the various regions of an antibody. First of all, the antibody has two different \"chains\", meaning chains of ammino acid - the light chain and the heavy chain. Further, each of these chains can be broken down into two parts - the constant region, and the variable region. For a given antibody type, the constant region remains exactly the same. Two antibodies targeting entirely different pathogens will express the same constant regions nonetheless. The true power of the antibody, however, rests in the variable region- this is the region that actually binds to a target pathogen. \n",
    "\n",
    "To build the variable region, the body's B cells randomly pull from a gene pool in order to introduce genetic variance. The amino acid sequence of the variable region, therefore, is unique to each specific antibody. The body can generate virtually endless amino acid sequences in the varible region of the antibodies it produces; therefore, when these B cells are attempting to generate an antibody that can successfully bind a pathogen, they can merelly mass-produce antibodies, introducing genetic variance each time, until a successfully binding antibody emerges. Taking this approach results in an extremely high probability that the body can bind a given pathenogen.\n",
    "\n",
    "#### Application to Project\n",
    "\n",
    "Because of the genetic variance of these variable regions, there are no easy ways to perform any kind of analysis on them without data-driven techniques. In the case of this project, we will be feeding an algorithm only the sequence of the variable region of the antibody light chain. In addition, to provide a little extra data, we will be providing the algorithm data about the substructure of this light chain. To be more specific, all variable regions can be broken down into a couple different pieces, namely the complimentary-determining (CDR) regions, and the framework regions. The framework regions are more genetically stable than the CDR regions, so the CDR regions are really the  piece of the antibody that is primarily responsible for binding specificity. CDRs and framworks, in any given antibody, are always found in the antibody sequence in the following order:\n",
    "\n",
    "- Framework 1, CDR1, Framework 2, CDR2, Framework 3, CDR3, Framework 4\n",
    "\n",
    "Therefore, to provide the algorithm with additional data, we will be providing it the sequences of the CDR1,2, and 3 that form a piece of its variable light chain.\n",
    "\n",
    "In this project, we will be feeding the algorithm mouse and human antibody sequences only, and attempting to enable the algorithm to identity the species using this sequence data.\n",
    "\n",
    "Although this may seem like an impossible task, given the genetic variability/ randomness of the variable region, but keep in mind that each species is pulling from a different set of genes to create this variabilty. Therefore, across species, similar sequence motifs are expressed. Having this knowledge, we will attempt to see if our algorithm can identify some of these motifs, and make a distinction based on species.\n",
    "\n",
    "## Downfalls:\n",
    "\n",
    "1. The dataset I used, the Kabat antibody dataset, is from the late 1990s, and is formatted very poorly for usage in machine learning. In addition, there were numerous empty datafields, forcing me to cut my dataset to a mere 1000 sequences\n",
    "\n",
    "2. As a result of this, an unbelievable ammount of my code is pure data preparation, and my voyage into machine learning here is pretty unreasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test efficacy of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data to be used in this project derives from the Kabat sequence database, the largest database of antibody sequences by protein sequence. This dataset includes multiple species and approx. 50,000 entires... however, many of these entries include incomplete data. Further, the database is neither easy to find nor easy to work with.\n",
    "\n",
    "To simplify things, I've done some preprocessing on the database to attain the csv files you see below in this notebook, but will be leveraging the data formatting power of pandas to remove any incomplete entries, format columns etc.\n",
    "\n",
    "For the first machine learning task, I will be using the follow data points:\n",
    "\n",
    "- Length CDRL1\n",
    "- Length CDRL2\n",
    "- Length CDRL3\n",
    "- A given sequence's % identity match, on average, with other human antibody sequences\n",
    "- % identity match, on average, with other mouse antibody sequences\n",
    "- % identity of the sequence's best single match with any human antibody in our database\n",
    "- % identity of the sequence's best single match with any mouse antibody in our database\n",
    "\n",
    "In this inital machine learning task, I won't be using the actual antibody sequence data itself, as this would require a convolutional neural net and a little more code. We'll leave that for later. For now, all I'll be doing with the antibody light chain sequences is generating identity matrixes-- indicating how similar a given antibody chain is to the antibody chains coming from other species.\n",
    "\n",
    "In order for our algorithm to distinguish between species, it will only need to understand the relationship between species-specific sequence identity, and the length of the CDRs.\n",
    "\n",
    "Thus, for this task, all data entries lacking sequence or CDR length data will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_data(in_file,out_file):\n",
    "    with open(in_file,\"r\") as input:\n",
    "        with open(out_file,\"w\") as output:\n",
    "            for line in input:\n",
    "                if not \"?\" in line and not \"X\" in line:\n",
    "                    output.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_data(\"./data/mouse_human.csv\",\"./data/mouse_human_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/mouse_human.csv\",\"r\") as input:\n",
    "    with open(\"./data/mouse_human_data.csv\",\"w\") as output:\n",
    "        for line in input:\n",
    "            if not \"?\" in line and not \"X\" in line:\n",
    "                output.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv file\n",
    "mAbs = pd.read_csv('./data/mouse_human_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>L3</th>\n",
       "      <th>LengthL1</th>\n",
       "      <th>LengthL2</th>\n",
       "      <th>LengthL3</th>\n",
       "      <th>Light</th>\n",
       "      <th>Heavy</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RASQSVSSSYLA</td>\n",
       "      <td>GASSRAT</td>\n",
       "      <td>QQYGSSPSIT</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKASGYTFTSYAMHWVRQAPGQRLE...</td>\n",
       "      <td>HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TGTSSDVGGYKYVS</td>\n",
       "      <td>DVTNRPS</td>\n",
       "      <td>SSYAGAQSLV</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>QSVLTQPASVSGSPDQSITISCTGTSSDVGGYKYVSWYQQHPDKAP...</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...</td>\n",
       "      <td>HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASSTGAVTTGYYPN</td>\n",
       "      <td>STSNKHS</td>\n",
       "      <td>LLYYGGA</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>QTVVTQQPSLTVSPGGTVTLTCASSTGAVTTGYYPNWFQQKPGQAP...</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYIHWVRQAPGQGLE...</td>\n",
       "      <td>HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KSSQSLLYSSNNKNYLA</td>\n",
       "      <td>WASTRES</td>\n",
       "      <td>QQYYSTPPM</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNKNYLAWYQQKP...</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCEASGYTFTGHYMHWVGQAPGQGLE...</td>\n",
       "      <td>HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RASQSIGSFLH</td>\n",
       "      <td>AASSLQS</td>\n",
       "      <td>QQSYSTLYT</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQSIGSFLHWYQQKPGKGPKL...</td>\n",
       "      <td>QVQLVQSGAEVKKPGASVKVSCKTSGYTFVAYYVHWVRQAPGQGLQ...</td>\n",
       "      <td>HUMAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  L1       L2          L3  LengthL1  LengthL2  LengthL3  \\\n",
       "0       RASQSVSSSYLA  GASSRAT  QQYGSSPSIT        12         7        10   \n",
       "1     TGTSSDVGGYKYVS  DVTNRPS  SSYAGAQSLV        14         7        10   \n",
       "2     ASSTGAVTTGYYPN  STSNKHS     LLYYGGA        14         7         7   \n",
       "3  KSSQSLLYSSNNKNYLA  WASTRES   QQYYSTPPM        17         7         9   \n",
       "4        RASQSIGSFLH  AASSLQS   QQSYSTLYT        11         7         9   \n",
       "\n",
       "                                               Light  \\\n",
       "0  EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...   \n",
       "1  QSVLTQPASVSGSPDQSITISCTGTSSDVGGYKYVSWYQQHPDKAP...   \n",
       "2  QTVVTQQPSLTVSPGGTVTLTCASSTGAVTTGYYPNWFQQKPGQAP...   \n",
       "3  DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNKNYLAWYQQKP...   \n",
       "4  DIQMTQSPSSLSASVGDRVTITCRASQSIGSFLHWYQQKPGKGPKL...   \n",
       "\n",
       "                                               Heavy Source  \n",
       "0  QVQLVQSGAEVKKPGASVKVSCKASGYTFTSYAMHWVRQAPGQRLE...  HUMAN  \n",
       "1  QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYMHWVRQAPGQGLE...  HUMAN  \n",
       "2  QVQLVQSGAEVKKPGASVKVSCKASGYTFTGYYIHWVRQAPGQGLE...  HUMAN  \n",
       "3  QVQLVQSGAEVKKPGASVKVSCEASGYTFTGHYMHWVGQAPGQGLE...  HUMAN  \n",
       "4  QVQLVQSGAEVKKPGASVKVSCKTSGYTFVAYYVHWVRQAPGQGLQ...  HUMAN  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mAbs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unneccessary Data Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAbs = mAbs.drop(['Heavy'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CDR2s are always 7 residues in length across this entire dataset, as is extremely common for mouse and human antibodies. Therefore, this data point is not useful to us and should be dropped out as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAbs = mAbs.drop(['LengthL2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>L3</th>\n",
       "      <th>LengthL1</th>\n",
       "      <th>LengthL3</th>\n",
       "      <th>Light</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RASQSVSSSYLA</td>\n",
       "      <td>GASSRAT</td>\n",
       "      <td>QQYGSSPSIT</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...</td>\n",
       "      <td>HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TGTSSDVGGYKYVS</td>\n",
       "      <td>DVTNRPS</td>\n",
       "      <td>SSYAGAQSLV</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>QSVLTQPASVSGSPDQSITISCTGTSSDVGGYKYVSWYQQHPDKAP...</td>\n",
       "      <td>HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASSTGAVTTGYYPN</td>\n",
       "      <td>STSNKHS</td>\n",
       "      <td>LLYYGGA</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>QTVVTQQPSLTVSPGGTVTLTCASSTGAVTTGYYPNWFQQKPGQAP...</td>\n",
       "      <td>HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KSSQSLLYSSNNKNYLA</td>\n",
       "      <td>WASTRES</td>\n",
       "      <td>QQYYSTPPM</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNKNYLAWYQQKP...</td>\n",
       "      <td>HUMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RASQSIGSFLH</td>\n",
       "      <td>AASSLQS</td>\n",
       "      <td>QQSYSTLYT</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQSIGSFLHWYQQKPGKGPKL...</td>\n",
       "      <td>HUMAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  L1       L2          L3  LengthL1  LengthL3  \\\n",
       "0       RASQSVSSSYLA  GASSRAT  QQYGSSPSIT        12        10   \n",
       "1     TGTSSDVGGYKYVS  DVTNRPS  SSYAGAQSLV        14        10   \n",
       "2     ASSTGAVTTGYYPN  STSNKHS     LLYYGGA        14         7   \n",
       "3  KSSQSLLYSSNNKNYLA  WASTRES   QQYYSTPPM        17         9   \n",
       "4        RASQSIGSFLH  AASSLQS   QQSYSTLYT        11         9   \n",
       "\n",
       "                                               Light Source  \n",
       "0  EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...  HUMAN  \n",
       "1  QSVLTQPASVSGSPDQSITISCTGTSSDVGGYKYVSWYQQHPDKAP...  HUMAN  \n",
       "2  QTVVTQQPSLTVSPGGTVTLTCASSTGAVTTGYYPNWFQQKPGQAP...  HUMAN  \n",
       "3  DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNKNYLAWYQQKP...  HUMAN  \n",
       "4  DIQMTQSPSSLSASVGDRVTITCRASQSIGSFLHWYQQKPGKGPKL...  HUMAN  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mAbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1106"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mAbs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Missing Data\n",
    "- Remove any entries without sequence data, using pandas dropna function\n",
    "(Missing entries default to Nan in pandas. Dropna removes any rows with missing entires)\n",
    "\n",
    "- Eliminate sequences with and CDR Lengths listed as zero, as this indicates missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAbs = mAbs.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAbs = mAbs[mAbs.LengthL1 != 0]\n",
    "mAbs = mAbs[mAbs.LengthL3 != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fasta sequences for alignment later on:\n",
    " - Later in the generation of the dataset, we'll need to create a percentage identity matrix using a program called clustal omega. This information will enable us to determine how well a given antibody Fv sequence matches with sequences from other species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAbs['Light'].to_csv('./data/seqs', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_gen(sequence_file):\n",
    "    i = 0\n",
    "    out_file = sequence_file + \".fasta\"\n",
    "    with open(sequence_file,\"r\") as input:\n",
    "        with open(out_file,\"w\") as output:\n",
    "            for line in input:\n",
    "                line = \">sequence\" + str(i) + \"\\n\" + line\n",
    "                output.write(line)\n",
    "                i += 1\n",
    "    os.remove(sequence_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_gen('./data/seqs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the identity matrix and species similarity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next piece of data generation involes determining how similar a given sequence is to other sequences generated by its species. In order to assign a metric to each sequence for a dataset, indicating how similar it is to either the mouse species or the human species, we will use the following procedure:\n",
    "\n",
    "1. We have now generated a set of light chain sequences for every antibody in our dataset. Therefore, we are now able to use tools for fasta processing\n",
    "\n",
    "2. The tool clustal omega, a protein sequence alignment tool created by the European Bioinformatics Institute. The tool, if fed a fasta file of sequences, will individually align and compare each sequence in the file to every other sequence\n",
    "\n",
    "3. The tool is able to generate an identity matrix, where one sequence's similarity to another's is expressed as a %match (i.e. how many of the residues match between the two sequences after a proper alignment has been made. In this identity matrix, an percentage entry at row1 column5 indicates sequence 1's similiarity \n",
    "\n",
    "4. Because this tool is extremely computationally expensive, we are going to run the tool on a server I rented out on the cloud to perform this project. \n",
    "\n",
    "`clustalo --threads=$(nproc) --in seqs.fa --full --distmat-out=percents.csv --percent-id --out seq_identity`\n",
    "\n",
    "5. Percents file defaults to using spaces as the delimiter... convert the `percents` file into a true csv file using sed command to replace spaces with commas. Now we are ready to process with python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.fasta.fasta": ">sequence0\n--EIVLTQSPGTLSLSPGERATLSCRASQSVSSS---------YLAWYQQKPGQAPRLLI\nYG----ASSRATGIPDRFSGSGS--G--------TDFTLTISRLEPADFAVYYCQQYGS-\nS-P-SITFGQGTRLEIKR--\n>sequence1\n--QSVLTQ-PASVSGSPDQSITISCTGTSSDVGGY-------KYVSWYQQHPDKAPKVMI\nYD----VTNRPSGGSNRFSGSKS--G--------NTASLTISGLQAEDEADYYCSSYAGA\nQS-L-VFG-GGTKLTVLG--\n>sequence2\n--QTVVTQ-QPSLTVSPGGTVTLTCASSTGAVTTG-------YYPNWFQQKPGQAPRALI\nYS----TSNKHSWTPARFSGSLL--G--------GKAALTLSGVQPEDEAEYFCLLYYGG\nA-------------------\n>sequence3\n--DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNK----NYLAWYQQKPGQPPKLLI\nYW----ASTRESGVPDRFSGSGS--G--------TDFTLTISSLQAEDVAVYYCQQYYS-\nT-P--PMFGQGTKVEIKRT-\n>sequence4\n--DIQMTQSPSSLSASVGDRVTITCRASQ------SIG----SFLHWYQQKPGKGPKLLI\nSA----ASSLQSGVPSRFSGSGS--G--------TDFTLTISSLQPEDFATYYCQQSYST\n--L--YTFGQGTKLEIKR--\n>sequence5\n--EIVLTQSPGTLSLSPGERATLSCRASQSISSS---------FLAWYQQKVGQAPRLLI\nYG----ASSRATGIPDRFSGSGS--G--------TDFTLTISRLEPEDFAVYYCQQYGS-\nS-P--LTFGGGTKVEIKR--\n",
      "text/plain": [
       ">sequence0\n",
       "--EIVLTQSPGTLSLSPGERATLSCRASQSVSSS---------YLAWYQQKPGQAPRLLI\n",
       "YG----ASSRATGIPDRFSGSGS--G--------TDFTLTISRLEPADFAVYYCQQYGS-\n",
       "S-P-SITFGQGTRLEIKR--\n",
       ">sequence1\n",
       "--QSVLTQ-PASVSGSPDQSITISCTGTSSDVGGY-------KYVSWYQQHPDKAPKVMI\n",
       "YD----VTNRPSGGSNRFSGSKS--G--------NTASLTISGLQAEDEADYYCSSYAGA\n",
       "QS-L-VFG-GGTKLTVLG--\n",
       ">sequence2\n",
       "--QTVVTQ-QPSLTVSPGGTVTLTCASSTGAVTTG-------YYPNWFQQKPGQAPRALI\n",
       "YS----TSNKHSWTPARFSGSLL--G--------GKAALTLSGVQPEDEAEYFCLLYYGG\n",
       "A-------------------\n",
       ">sequence3\n",
       "--DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNK----NYLAWYQQKPGQPPKLLI\n",
       "YW----ASTRESGVPDRFSGSGS--G--------TDFTLTISSLQAEDVAVYYCQQYYS-\n",
       "T-P--PMFGQGTKVEIKRT-\n",
       ">sequence4\n",
       "--DIQMTQSPSSLSASVGDRVTITCRASQ------SIG----SFLHWYQQKPGKGPKLLI\n",
       "SA----ASSLQSGVPSRFSGSGS--G--------TDFTLTISSLQPEDFATYYCQQSYST\n",
       "--L--YTFGQGTKLEIKR--\n",
       ">sequence5\n",
       "--EIVLTQSPGTLSLSPGERATLSCRASQSISSS---------FLAWYQQKVGQAPRLLI\n",
       "YG----ASSRATGIPDRFSGSGS--G--------TDFTLTISRLEPEDFAVYYCQQYGS-\n",
       "S-P--LTFGGGTKVEIKR--\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To give you an idea of how clustal is aligning the sequences and assigns % identity\n",
    "# you can visualize how it introduces gaps to help align sequences.\n",
    "# Below are some of the sequences from our dataset that clustal aligned for us:\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "def Fasta(data='f'):\n",
    "    bundle = {}\n",
    "    bundle['application/vnd.fasta.fasta'] = data\n",
    "    bundle['text/plain'] = data\n",
    "    display(bundle, raw=True)\n",
    "\n",
    "Fasta(\"\"\">sequence0\n",
    "--EIVLTQSPGTLSLSPGERATLSCRASQSVSSS---------YLAWYQQKPGQAPRLLI\n",
    "YG----ASSRATGIPDRFSGSGS--G--------TDFTLTISRLEPADFAVYYCQQYGS-\n",
    "S-P-SITFGQGTRLEIKR--\n",
    ">sequence1\n",
    "--QSVLTQ-PASVSGSPDQSITISCTGTSSDVGGY-------KYVSWYQQHPDKAPKVMI\n",
    "YD----VTNRPSGGSNRFSGSKS--G--------NTASLTISGLQAEDEADYYCSSYAGA\n",
    "QS-L-VFG-GGTKLTVLG--\n",
    ">sequence2\n",
    "--QTVVTQ-QPSLTVSPGGTVTLTCASSTGAVTTG-------YYPNWFQQKPGQAPRALI\n",
    "YS----TSNKHSWTPARFSGSLL--G--------GKAALTLSGVQPEDEAEYFCLLYYGG\n",
    "A-------------------\n",
    ">sequence3\n",
    "--DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNK----NYLAWYQQKPGQPPKLLI\n",
    "YW----ASTRESGVPDRFSGSGS--G--------TDFTLTISSLQAEDVAVYYCQQYYS-\n",
    "T-P--PMFGQGTKVEIKRT-\n",
    ">sequence4\n",
    "--DIQMTQSPSSLSASVGDRVTITCRASQ------SIG----SFLHWYQQKPGKGPKLLI\n",
    "SA----ASSLQSGVPSRFSGSGS--G--------TDFTLTISSLQPEDFATYYCQQSYST\n",
    "--L--YTFGQGTKLEIKR--\n",
    ">sequence5\n",
    "--EIVLTQSPGTLSLSPGERATLSCRASQSISSS---------FLAWYQQKVGQAPRLLI\n",
    "YG----ASSRATGIPDRFSGSGS--G--------TDFTLTISRLEPEDFAVYYCQQYGS-\n",
    "S-P--LTFGGGTKVEIKR--\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_matrix = np.array(list(csv.reader(open(\"./data/percents.csv\", \"r\"), delimiter=\",\"))).astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to percentages only, then change string representation of values to floats\n",
    "id_matrix = id_matrix[:,1:].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1106, 1106)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_matrix.shape\n",
    "# Extra column comes from the sequence name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First entries in database are human. Get number of entries for demonstration\n",
    "len(mAbs[mAbs.Source == 'HUMAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_humans = len(mAbs[mAbs.Source == 'HUMAN'])\n",
    "num_mice = len(mAbs[mAbs.Source == 'MOUSE'])\n",
    "matches = open('./data/matches.csv','w')\n",
    "\n",
    "matches.write(\"Avg-Human-Match,Best-Human-Match,Avg-Mouse-Match,Best-Mouse-Match\\n\")\n",
    "\n",
    "for i in range(0,np.shape(id_matrix)[0]):\n",
    "    \n",
    "    human_match = 0\n",
    "    mouse_match = 0\n",
    "    best_human_match = 0\n",
    "    best_mouse_match = 0\n",
    "    \n",
    "    for j in range(0,np.shape(id_matrix)[1]):\n",
    "        \n",
    "        if j < num_humans and id_matrix.item((i,j)) != 100.0 :\n",
    "            human_match += id_matrix.item((i,j))\n",
    "            \n",
    "            if id_matrix.item((i,j)) > best_human_match:\n",
    "                best_human_match = id_matrix.item((i,j))\n",
    "            \n",
    "        \n",
    "        elif id_matrix.item((i,j)) != 100 :\n",
    "            mouse_match += id_matrix.item((i,j))\n",
    "            \n",
    "            if id_matrix.item((i,j)) > best_mouse_match:\n",
    "                best_mouse_match = id_matrix.item((i,j))\n",
    "            \n",
    "    human_match /= num_humans -1 \n",
    "    mouse_match /= num_mice -1 \n",
    "        \n",
    "    matches.write(str(human_match) + \",\" + str(best_human_match) + \",\" + \\\n",
    "                  str(mouse_match) + \",\" + str(best_mouse_match) + '\\n')\n",
    "\n",
    "matches.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_vals = pd.read_csv('./data/matches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add everything into prexisting dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAbs['AvgHumanMatch'] = match_vals['Avg-Human-Match']\n",
    "mAbs['BestHumanMatch'] = match_vals['Best-Human-Match']\n",
    "mAbs['AvgMouseMatch'] = match_vals['Avg-Mouse-Match']\n",
    "mAbs['BestMouseMatch'] = match_vals['Best-Mouse-Match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>L3</th>\n",
       "      <th>LengthL1</th>\n",
       "      <th>LengthL3</th>\n",
       "      <th>Light</th>\n",
       "      <th>Source</th>\n",
       "      <th>AvgHumanMatch</th>\n",
       "      <th>BestHumanMatch</th>\n",
       "      <th>AvgMouseMatch</th>\n",
       "      <th>BestMouseMatch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RASQSVSSSYLA</td>\n",
       "      <td>GASSRAT</td>\n",
       "      <td>QQYGSSPSIT</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>58.033865</td>\n",
       "      <td>98.958333</td>\n",
       "      <td>54.246754</td>\n",
       "      <td>66.972477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TGTSSDVGGYKYVS</td>\n",
       "      <td>DVTNRPS</td>\n",
       "      <td>SSYAGAQSLV</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>QSVLTQPASVSGSPDQSITISCTGTSSDVGGYKYVSWYQQHPDKAP...</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>44.926556</td>\n",
       "      <td>87.387387</td>\n",
       "      <td>35.748652</td>\n",
       "      <td>44.036697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASSTGAVTTGYYPN</td>\n",
       "      <td>STSNKHS</td>\n",
       "      <td>LLYYGGA</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>QTVVTQQPSLTVSPGGTVTLTCASSTGAVTTGYYPNWFQQKPGQAP...</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>38.934404</td>\n",
       "      <td>62.886598</td>\n",
       "      <td>36.799879</td>\n",
       "      <td>60.824742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KSSQSLLYSSNNKNYLA</td>\n",
       "      <td>WASTRES</td>\n",
       "      <td>QQYYSTPPM</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNKNYLAWYQQKP...</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>58.445212</td>\n",
       "      <td>98.260870</td>\n",
       "      <td>59.885397</td>\n",
       "      <td>83.185841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RASQSIGSFLH</td>\n",
       "      <td>AASSLQS</td>\n",
       "      <td>QQSYSTLYT</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQSIGSFLHWYQQKPGKGPKL...</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>60.007143</td>\n",
       "      <td>92.523364</td>\n",
       "      <td>60.291405</td>\n",
       "      <td>78.703704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  L1       L2          L3  LengthL1  LengthL3  \\\n",
       "0       RASQSVSSSYLA  GASSRAT  QQYGSSPSIT        12        10   \n",
       "1     TGTSSDVGGYKYVS  DVTNRPS  SSYAGAQSLV        14        10   \n",
       "2     ASSTGAVTTGYYPN  STSNKHS     LLYYGGA        14         7   \n",
       "3  KSSQSLLYSSNNKNYLA  WASTRES   QQYYSTPPM        17         9   \n",
       "4        RASQSIGSFLH  AASSLQS   QQSYSTLYT        11         9   \n",
       "\n",
       "                                               Light Source  AvgHumanMatch  \\\n",
       "0  EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...  HUMAN      58.033865   \n",
       "1  QSVLTQPASVSGSPDQSITISCTGTSSDVGGYKYVSWYQQHPDKAP...  HUMAN      44.926556   \n",
       "2  QTVVTQQPSLTVSPGGTVTLTCASSTGAVTTGYYPNWFQQKPGQAP...  HUMAN      38.934404   \n",
       "3  DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNKNYLAWYQQKP...  HUMAN      58.445212   \n",
       "4  DIQMTQSPSSLSASVGDRVTITCRASQSIGSFLHWYQQKPGKGPKL...  HUMAN      60.007143   \n",
       "\n",
       "   BestHumanMatch  AvgMouseMatch  BestMouseMatch  \n",
       "0       98.958333      54.246754       66.972477  \n",
       "1       87.387387      35.748652       44.036697  \n",
       "2       62.886598      36.799879       60.824742  \n",
       "3       98.260870      59.885397       83.185841  \n",
       "4       92.523364      60.291405       78.703704  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mAbs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data types into correct format:\n",
    "- String representations of numerical values -> floats\n",
    "- Source to a binary representation, either 1 for 0, where 1 is human, and 0 is mouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_float = ['LengthL1','LengthL3','AvgHumanMatch','AvgMouseMatch','BestHumanMatch','BestMouseMatch']\n",
    "mAbs[cols_to_float] = mAbs[cols_to_float].astype(float)\n",
    "\n",
    "# Convert species classifications to learnable output:\n",
    "mAbs['Source'] = mAbs['Source'].apply(lambda label: int(label == 'MOUSE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>L3</th>\n",
       "      <th>LengthL1</th>\n",
       "      <th>LengthL3</th>\n",
       "      <th>Light</th>\n",
       "      <th>Source</th>\n",
       "      <th>AvgHumanMatch</th>\n",
       "      <th>BestHumanMatch</th>\n",
       "      <th>AvgMouseMatch</th>\n",
       "      <th>BestMouseMatch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RASQSVSSSYLA</td>\n",
       "      <td>GASSRAT</td>\n",
       "      <td>QQYGSSPSIT</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...</td>\n",
       "      <td>0</td>\n",
       "      <td>58.033865</td>\n",
       "      <td>98.958333</td>\n",
       "      <td>54.246754</td>\n",
       "      <td>66.972477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TGTSSDVGGYKYVS</td>\n",
       "      <td>DVTNRPS</td>\n",
       "      <td>SSYAGAQSLV</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>QSVLTQPASVSGSPDQSITISCTGTSSDVGGYKYVSWYQQHPDKAP...</td>\n",
       "      <td>0</td>\n",
       "      <td>44.926556</td>\n",
       "      <td>87.387387</td>\n",
       "      <td>35.748652</td>\n",
       "      <td>44.036697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASSTGAVTTGYYPN</td>\n",
       "      <td>STSNKHS</td>\n",
       "      <td>LLYYGGA</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>QTVVTQQPSLTVSPGGTVTLTCASSTGAVTTGYYPNWFQQKPGQAP...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.934404</td>\n",
       "      <td>62.886598</td>\n",
       "      <td>36.799879</td>\n",
       "      <td>60.824742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KSSQSLLYSSNNKNYLA</td>\n",
       "      <td>WASTRES</td>\n",
       "      <td>QQYYSTPPM</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNKNYLAWYQQKP...</td>\n",
       "      <td>0</td>\n",
       "      <td>58.445212</td>\n",
       "      <td>98.260870</td>\n",
       "      <td>59.885397</td>\n",
       "      <td>83.185841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RASQSIGSFLH</td>\n",
       "      <td>AASSLQS</td>\n",
       "      <td>QQSYSTLYT</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQSIGSFLHWYQQKPGKGPKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>60.007143</td>\n",
       "      <td>92.523364</td>\n",
       "      <td>60.291405</td>\n",
       "      <td>78.703704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  L1       L2          L3  LengthL1  LengthL3  \\\n",
       "0       RASQSVSSSYLA  GASSRAT  QQYGSSPSIT      12.0      10.0   \n",
       "1     TGTSSDVGGYKYVS  DVTNRPS  SSYAGAQSLV      14.0      10.0   \n",
       "2     ASSTGAVTTGYYPN  STSNKHS     LLYYGGA      14.0       7.0   \n",
       "3  KSSQSLLYSSNNKNYLA  WASTRES   QQYYSTPPM      17.0       9.0   \n",
       "4        RASQSIGSFLH  AASSLQS   QQSYSTLYT      11.0       9.0   \n",
       "\n",
       "                                               Light  Source  AvgHumanMatch  \\\n",
       "0  EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...       0      58.033865   \n",
       "1  QSVLTQPASVSGSPDQSITISCTGTSSDVGGYKYVSWYQQHPDKAP...       0      44.926556   \n",
       "2  QTVVTQQPSLTVSPGGTVTLTCASSTGAVTTGYYPNWFQQKPGQAP...       0      38.934404   \n",
       "3  DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNKNYLAWYQQKP...       0      58.445212   \n",
       "4  DIQMTQSPSSLSASVGDRVTITCRASQSIGSFLHWYQQKPGKGPKL...       0      60.007143   \n",
       "\n",
       "   BestHumanMatch  AvgMouseMatch  BestMouseMatch  \n",
       "0       98.958333      54.246754       66.972477  \n",
       "1       87.387387      35.748652       44.036697  \n",
       "2       62.886598      36.799879       60.824742  \n",
       "3       98.260870      59.885397       83.185841  \n",
       "4       92.523364      60.291405       78.703704  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mAbs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform normalization:\n",
    "- In order to prevent any of our data points from having an undue influence on our model, we will now\n",
    "normalize each of the data points.\n",
    "- Since the percentage values are significantly different in magnitude than the CDR length values, a normalization is neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = ['LengthL1','LengthL3','AvgHumanMatch','AvgMouseMatch','BestHumanMatch','BestMouseMatch']\n",
    "mAbs[cols_to_normalize] = mAbs[cols_to_normalize].apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>L3</th>\n",
       "      <th>LengthL1</th>\n",
       "      <th>LengthL3</th>\n",
       "      <th>Light</th>\n",
       "      <th>Source</th>\n",
       "      <th>AvgHumanMatch</th>\n",
       "      <th>BestHumanMatch</th>\n",
       "      <th>AvgMouseMatch</th>\n",
       "      <th>BestMouseMatch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RASQSVSSSYLA</td>\n",
       "      <td>GASSRAT</td>\n",
       "      <td>QQYGSSPSIT</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833534</td>\n",
       "      <td>0.996809</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.470002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TGTSSDVGGYKYVS</td>\n",
       "      <td>DVTNRPS</td>\n",
       "      <td>SSYAGAQSLV</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>QSVLTQPASVSGSPDQSITISCTGTSSDVGGYKYVSWYQQHPDKAP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.377655</td>\n",
       "      <td>0.761221</td>\n",
       "      <td>0.166577</td>\n",
       "      <td>0.091906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASSTGAVTTGYYPN</td>\n",
       "      <td>STSNKHS</td>\n",
       "      <td>LLYYGGA</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>QTVVTQQPSLTVSPGGTVTLTCASSTGAVTTGYYPNWFQQKPGQAP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169244</td>\n",
       "      <td>0.262376</td>\n",
       "      <td>0.199547</td>\n",
       "      <td>0.368657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KSSQSLLYSSNNKNYLA</td>\n",
       "      <td>WASTRES</td>\n",
       "      <td>QQYYSTPPM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNKNYLAWYQQKP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.847841</td>\n",
       "      <td>0.982609</td>\n",
       "      <td>0.923587</td>\n",
       "      <td>0.737279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RASQSIGSFLH</td>\n",
       "      <td>AASSLQS</td>\n",
       "      <td>QQSYSTLYT</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCRASQSIGSFLHWYQQKPGKGPKL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902165</td>\n",
       "      <td>0.865791</td>\n",
       "      <td>0.936321</td>\n",
       "      <td>0.663391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  L1       L2          L3  LengthL1  LengthL3  \\\n",
       "0       RASQSVSSSYLA  GASSRAT  QQYGSSPSIT  0.285714  0.727273   \n",
       "1     TGTSSDVGGYKYVS  DVTNRPS  SSYAGAQSLV  0.571429  0.727273   \n",
       "2     ASSTGAVTTGYYPN  STSNKHS     LLYYGGA  0.571429  0.454545   \n",
       "3  KSSQSLLYSSNNKNYLA  WASTRES   QQYYSTPPM  1.000000  0.636364   \n",
       "4        RASQSIGSFLH  AASSLQS   QQSYSTLYT  0.142857  0.636364   \n",
       "\n",
       "                                               Light  Source  AvgHumanMatch  \\\n",
       "0  EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPR...       0       0.833534   \n",
       "1  QSVLTQPASVSGSPDQSITISCTGTSSDVGGYKYVSWYQQHPDKAP...       0       0.377655   \n",
       "2  QTVVTQQPSLTVSPGGTVTLTCASSTGAVTTGYYPNWFQQKPGQAP...       0       0.169244   \n",
       "3  DIVMTQSPDSLAVSLGERATINCKSSQSLLYSSNNKNYLAWYQQKP...       0       0.847841   \n",
       "4  DIQMTQSPSSLSASVGDRVTITCRASQSIGSFLHWYQQKPGKGPKL...       0       0.902165   \n",
       "\n",
       "   BestHumanMatch  AvgMouseMatch  BestMouseMatch  \n",
       "0        0.996809       0.746740        0.470002  \n",
       "1        0.761221       0.166577        0.091906  \n",
       "2        0.262376       0.199547        0.368657  \n",
       "3        0.982609       0.923587        0.737279  \n",
       "4        0.865791       0.936321        0.663391  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mAbs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Sequence values until later\n",
    "Since we won't actually be using the sequences themselves, we'll drop these out of the dataset for now, and store them for later in case we have time to run a convolutional neural net that takes sequence data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mAbs= mAbs\n",
    "mAbs = mAbs.drop(['L1','L2','L3','Light'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LengthL1</th>\n",
       "      <th>LengthL3</th>\n",
       "      <th>Source</th>\n",
       "      <th>AvgHumanMatch</th>\n",
       "      <th>BestHumanMatch</th>\n",
       "      <th>AvgMouseMatch</th>\n",
       "      <th>BestMouseMatch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833534</td>\n",
       "      <td>0.996809</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.470002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0</td>\n",
       "      <td>0.377655</td>\n",
       "      <td>0.761221</td>\n",
       "      <td>0.166577</td>\n",
       "      <td>0.091906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169244</td>\n",
       "      <td>0.262376</td>\n",
       "      <td>0.199547</td>\n",
       "      <td>0.368657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.847841</td>\n",
       "      <td>0.982609</td>\n",
       "      <td>0.923587</td>\n",
       "      <td>0.737279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902165</td>\n",
       "      <td>0.865791</td>\n",
       "      <td>0.936321</td>\n",
       "      <td>0.663391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LengthL1  LengthL3  Source  AvgHumanMatch  BestHumanMatch  AvgMouseMatch  \\\n",
       "0  0.285714  0.727273       0       0.833534        0.996809       0.746740   \n",
       "1  0.571429  0.727273       0       0.377655        0.761221       0.166577   \n",
       "2  0.571429  0.454545       0       0.169244        0.262376       0.199547   \n",
       "3  1.000000  0.636364       0       0.847841        0.982609       0.923587   \n",
       "4  0.142857  0.636364       0       0.902165        0.865791       0.936321   \n",
       "\n",
       "   BestMouseMatch  \n",
       "0        0.470002  \n",
       "1        0.091906  \n",
       "2        0.368657  \n",
       "3        0.737279  \n",
       "4        0.663391  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mAbs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a train-test split\n",
    "- Use approx. 30% of the training data\n",
    "- The train test split algorithm from scikitlearn will randomize the data order for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x values for our net\n",
    "x_data = mAbs.drop('Source', axis = 1)\n",
    "\n",
    "# t values for our net\n",
    "y_labels = mAbs['Source']\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_labels,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_labels,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous feature columns\n",
    "len_l1 = tf.feature_column.numeric_column(\"LengthL1\")\n",
    "len_l3 = tf.feature_column.numeric_column(\"LengthL3\")\n",
    "avg_human_match  = tf.feature_column.numeric_column(\"AvgHumanMatch\")\n",
    "avg_mouse_match  = tf.feature_column.numeric_column(\"AvgMouseMatch\")\n",
    "best_human_match = tf.feature_column.numeric_column(\"BestHumanMatch\")\n",
    "best_mouse_match = tf.feature_column.numeric_column(\"BestMouseMatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [len_l1,len_l3,avg_human_match,avg_mouse_match,best_human_match,best_mouse_match]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Model\n",
    "- At this point in time, we have all of the data we need to generate the model\n",
    "- Tensorflow will abstract the majority of this process for us, and we will be using a simple linear classifier connected from the tensorflow estimator class to complete the process. In our class's terms, this linear classifier is merely an extremely simple perceptron.\n",
    "- According to [Keskar et. al. 2016](https://arxiv.org/abs/1609.04836), large batch sizes tend to lead to poor generalization of the model. The reasons behind this phenomena are somewhat unclear, but the truth remains nonetheless. Reccomended batch size is typically around 32 points for a dataset of around 1000 points, like the one we are using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_func = tf.estimator.inputs.pandas_input_fn(x=X_train, y=y_train, batch_size=32, num_epochs=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpqwowldas\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_session_config': None, '_tf_random_seed': 1, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_model_dir': '/tmp/tmpqwowldas', '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.LinearClassifier(feature_columns=feature_cols,n_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpqwowldas/model.ckpt.\n",
      "INFO:tensorflow:loss = 22.18071, step = 1\n",
      "INFO:tensorflow:global_step/sec: 265.909\n",
      "INFO:tensorflow:loss = 21.2858, step = 101 (0.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 466.134\n",
      "INFO:tensorflow:loss = 22.184834, step = 201 (0.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.193\n",
      "INFO:tensorflow:loss = 18.726091, step = 301 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.585\n",
      "INFO:tensorflow:loss = 19.113255, step = 401 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 418.078\n",
      "INFO:tensorflow:loss = 20.39795, step = 501 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 392.169\n",
      "INFO:tensorflow:loss = 21.27771, step = 601 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 507.071\n",
      "INFO:tensorflow:loss = 20.146925, step = 701 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.214\n",
      "INFO:tensorflow:loss = 19.58311, step = 801 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.158\n",
      "INFO:tensorflow:loss = 22.341251, step = 901 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 448.779\n",
      "INFO:tensorflow:loss = 17.783781, step = 1001 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.648\n",
      "INFO:tensorflow:loss = 21.840412, step = 1101 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.583\n",
      "INFO:tensorflow:loss = 21.048798, step = 1201 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 434.935\n",
      "INFO:tensorflow:loss = 20.088026, step = 1301 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.508\n",
      "INFO:tensorflow:loss = 16.921919, step = 1401 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.463\n",
      "INFO:tensorflow:loss = 19.810001, step = 1501 (0.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.928\n",
      "INFO:tensorflow:loss = 18.604963, step = 1601 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 440.688\n",
      "INFO:tensorflow:loss = 20.892246, step = 1701 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.9\n",
      "INFO:tensorflow:loss = 20.180983, step = 1801 (0.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 302.506\n",
      "INFO:tensorflow:loss = 19.200306, step = 1901 (0.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.277\n",
      "INFO:tensorflow:loss = 22.68872, step = 2001 (0.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 328.253\n",
      "INFO:tensorflow:loss = 20.014042, step = 2101 (0.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.682\n",
      "INFO:tensorflow:loss = 19.511614, step = 2201 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 477.551\n",
      "INFO:tensorflow:loss = 18.155615, step = 2301 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.326\n",
      "INFO:tensorflow:loss = 17.0057, step = 2401 (0.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.248\n",
      "INFO:tensorflow:loss = 18.583824, step = 2501 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 378.398\n",
      "INFO:tensorflow:loss = 18.745113, step = 2601 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.004\n",
      "INFO:tensorflow:loss = 19.513912, step = 2701 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.672\n",
      "INFO:tensorflow:loss = 20.719818, step = 2801 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.454\n",
      "INFO:tensorflow:loss = 15.607132, step = 2901 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.226\n",
      "INFO:tensorflow:loss = 18.884491, step = 3001 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 354.285\n",
      "INFO:tensorflow:loss = 16.983183, step = 3101 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 410.123\n",
      "INFO:tensorflow:loss = 16.095213, step = 3201 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.673\n",
      "INFO:tensorflow:loss = 20.195194, step = 3301 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.907\n",
      "INFO:tensorflow:loss = 20.473728, step = 3401 (0.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.069\n",
      "INFO:tensorflow:loss = 22.067879, step = 3501 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 387.509\n",
      "INFO:tensorflow:loss = 19.488525, step = 3601 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 385.861\n",
      "INFO:tensorflow:loss = 19.795315, step = 3701 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 550.858\n",
      "INFO:tensorflow:loss = 15.69285, step = 3801 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 428.095\n",
      "INFO:tensorflow:loss = 21.791088, step = 3901 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.436\n",
      "INFO:tensorflow:loss = 18.18115, step = 4001 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 326.539\n",
      "INFO:tensorflow:loss = 20.613346, step = 4101 (0.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.221\n",
      "INFO:tensorflow:loss = 19.52083, step = 4201 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 532.929\n",
      "INFO:tensorflow:loss = 20.497229, step = 4301 (0.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 450.224\n",
      "INFO:tensorflow:loss = 19.58841, step = 4401 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.016\n",
      "INFO:tensorflow:loss = 14.236017, step = 4501 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 414.687\n",
      "INFO:tensorflow:loss = 20.372034, step = 4601 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.544\n",
      "INFO:tensorflow:loss = 19.27055, step = 4701 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 382.968\n",
      "INFO:tensorflow:loss = 22.190142, step = 4801 (0.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.746\n",
      "INFO:tensorflow:loss = 16.25922, step = 4901 (0.251 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmpqwowldas/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 20.238886.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x7f92e3bb3390>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn=input_func, steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_func = tf.estimator.inputs.pandas_input_fn(\n",
    "      x=X_test,\n",
    "      y=y_test,\n",
    "      batch_size=32,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-14-04:05:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqwowldas/model.ckpt-5000\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-14-04:05:35\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.7801205, accuracy_baseline = 0.64759034, auc = 0.65122247, auc_precision_recall = 0.7003074, average_loss = 0.6029086, global_step = 5000, label/mean = 0.64759034, loss = 18.196877, prediction/mean = 0.6450904\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(eval_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7801205,\n",
       " 'accuracy_baseline': 0.64759034,\n",
       " 'auc': 0.65122247,\n",
       " 'auc_precision_recall': 0.7003074,\n",
       " 'average_loss': 0.6029086,\n",
       " 'global_step': 5000,\n",
       " 'label/mean': 0.64759034,\n",
       " 'loss': 18.196877,\n",
       " 'prediction/mean': 0.6450904}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions:\n",
    "pred_input_func = tf.estimator.inputs.pandas_input_fn(\n",
    "      x=X_test,\n",
    "      batch_size=32,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "predictions = model.predict(pred_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqwowldas/model.ckpt-5000\n"
     ]
    }
   ],
   "source": [
    "final_preds = []\n",
    "for pred in predictions:\n",
    "    final_preds.append(pred['class_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.39      0.56       117\n",
      "           1       0.75      0.99      0.85       215\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       332\n",
      "   macro avg       0.85      0.69      0.71       332\n",
      "weighted avg       0.82      0.78      0.75       332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Linear Classifier:\n",
    "- Clearly, using all of the information in the percent identity matries, combined with all of the information about CDR lengths, the model could determine classification fairly easily. - - \n",
    "\n",
    "- It's important to note that, even if accuracy is high, a model could be performing poorly due to a high count of false positives. Seeing that our `precision` is high (indicating that we have a very small fraction of false positives), and that our `recall` is high (indicating that we correctly predicted a high percentage of true positives)... we can be sufficiently confident that our model is solid.\n",
    "\n",
    "- This result is expected, given that sequences often express higher similarity to other sequences from their own species (unsuprisingly).\n",
    "\n",
    " \n",
    "## Data Deprivation\n",
    "Seeing now that the problem was extremely easy for the algorithm to solve, we will several of the feature columns and see if this changes our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [len_l1,len_l3,avg_human_match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpj2b6yulj\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_session_config': None, '_tf_random_seed': 1, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_model_dir': '/tmp/tmpj2b6yulj', '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.estimator.LinearClassifier(feature_columns=feature_cols,n_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpj2b6yulj/model.ckpt.\n",
      "INFO:tensorflow:loss = 22.18071, step = 1\n",
      "INFO:tensorflow:global_step/sec: 371.755\n",
      "INFO:tensorflow:loss = 22.184315, step = 101 (0.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.515\n",
      "INFO:tensorflow:loss = 22.137691, step = 201 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 406.87\n",
      "INFO:tensorflow:loss = 20.562845, step = 301 (0.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.457\n",
      "INFO:tensorflow:loss = 19.9204, step = 401 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 532.356\n",
      "INFO:tensorflow:loss = 17.914536, step = 501 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 351.285\n",
      "INFO:tensorflow:loss = 18.90388, step = 601 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 376.42\n",
      "INFO:tensorflow:loss = 17.279812, step = 701 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.647\n",
      "INFO:tensorflow:loss = 19.285978, step = 801 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 352.139\n",
      "INFO:tensorflow:loss = 21.036993, step = 901 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 385.795\n",
      "INFO:tensorflow:loss = 17.462349, step = 1001 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.44\n",
      "INFO:tensorflow:loss = 20.506561, step = 1101 (0.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 386.279\n",
      "INFO:tensorflow:loss = 20.405193, step = 1201 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 427.328\n",
      "INFO:tensorflow:loss = 14.88256, step = 1301 (0.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 526.833\n",
      "INFO:tensorflow:loss = 16.30218, step = 1401 (0.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 433.974\n",
      "INFO:tensorflow:loss = 21.747864, step = 1501 (0.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.608\n",
      "INFO:tensorflow:loss = 18.326138, step = 1601 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 390.036\n",
      "INFO:tensorflow:loss = 19.467707, step = 1701 (0.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 424.756\n",
      "INFO:tensorflow:loss = 20.221855, step = 1801 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 427.657\n",
      "INFO:tensorflow:loss = 19.706165, step = 1901 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 421.806\n",
      "INFO:tensorflow:loss = 18.622402, step = 2001 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.577\n",
      "INFO:tensorflow:loss = 20.271164, step = 2101 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 402.341\n",
      "INFO:tensorflow:loss = 22.8369, step = 2201 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.278\n",
      "INFO:tensorflow:loss = 21.074568, step = 2301 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 405.618\n",
      "INFO:tensorflow:loss = 18.454151, step = 2401 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 380\n",
      "INFO:tensorflow:loss = 19.197693, step = 2501 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.797\n",
      "INFO:tensorflow:loss = 20.002977, step = 2601 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 499.368\n",
      "INFO:tensorflow:loss = 18.337975, step = 2701 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 491.182\n",
      "INFO:tensorflow:loss = 18.962856, step = 2801 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 393.501\n",
      "INFO:tensorflow:loss = 18.463223, step = 2901 (0.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 420.239\n",
      "INFO:tensorflow:loss = 17.212929, step = 3001 (0.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.371\n",
      "INFO:tensorflow:loss = 18.735947, step = 3101 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 353.334\n",
      "INFO:tensorflow:loss = 18.700932, step = 3201 (0.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 416.712\n",
      "INFO:tensorflow:loss = 16.804989, step = 3301 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 373.778\n",
      "INFO:tensorflow:loss = 17.874067, step = 3401 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 514.716\n",
      "INFO:tensorflow:loss = 17.707909, step = 3501 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 346.795\n",
      "INFO:tensorflow:loss = 19.86174, step = 3601 (0.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.056\n",
      "INFO:tensorflow:loss = 18.5316, step = 3701 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 365.52\n",
      "INFO:tensorflow:loss = 24.698448, step = 3801 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.584\n",
      "INFO:tensorflow:loss = 14.16238, step = 3901 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 509.843\n",
      "INFO:tensorflow:loss = 18.293327, step = 4001 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 416.322\n",
      "INFO:tensorflow:loss = 19.19395, step = 4101 (0.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 448.143\n",
      "INFO:tensorflow:loss = 16.046066, step = 4201 (0.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 391.413\n",
      "INFO:tensorflow:loss = 19.565079, step = 4301 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 407.222\n",
      "INFO:tensorflow:loss = 21.15626, step = 4401 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.885\n",
      "INFO:tensorflow:loss = 20.62435, step = 4501 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.877\n",
      "INFO:tensorflow:loss = 20.4558, step = 4601 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 399.826\n",
      "INFO:tensorflow:loss = 17.779572, step = 4701 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.898\n",
      "INFO:tensorflow:loss = 17.602615, step = 4801 (0.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.409\n",
      "INFO:tensorflow:loss = 16.76052, step = 4901 (0.237 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmpj2b6yulj/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 15.886691.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x7f92e3ee9e48>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.train(input_fn=input_func, steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-14-04:05:49\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpj2b6yulj/model.ckpt-5000\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-14-04:05:49\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.7801205, accuracy_baseline = 0.64759034, auc = 0.650328, auc_precision_recall = 0.70007265, average_loss = 0.60311973, global_step = 5000, label/mean = 0.64759034, loss = 18.203249, prediction/mean = 0.65234005\n"
     ]
    }
   ],
   "source": [
    "results = model2.evaluate(eval_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7801205,\n",
       " 'accuracy_baseline': 0.64759034,\n",
       " 'auc': 0.650328,\n",
       " 'auc_precision_recall': 0.70007265,\n",
       " 'average_loss': 0.60311973,\n",
       " 'global_step': 5000,\n",
       " 'label/mean': 0.64759034,\n",
       " 'loss': 18.203249,\n",
       " 'prediction/mean': 0.65234005}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions:\n",
    "predictions = model2.predict(pred_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpj2b6yulj/model.ckpt-5000\n"
     ]
    }
   ],
   "source": [
    "final_preds = []\n",
    "for pred in predictions:\n",
    "    final_preds.append(pred['class_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.39      0.56       117\n",
      "           1       0.75      0.99      0.85       215\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       332\n",
      "   macro avg       0.85      0.69      0.71       332\n",
      "weighted avg       0.82      0.78      0.75       332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprived Perceptron Review:\n",
    "Here, it's pretty clear that our model suffered from the lack of data. As one last excercise, let's see how a densely connected neural net could do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmplndq1w0t\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_session_config': None, '_tf_random_seed': 1, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_model_dir': '/tmp/tmplndq1w0t', '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    }
   ],
   "source": [
    "dnn_model = tf.estimator.DNNClassifier(hidden_units=[4,8,6],feature_columns=feature_cols,n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmplndq1w0t/model.ckpt.\n",
      "INFO:tensorflow:loss = 22.486422, step = 1\n",
      "INFO:tensorflow:global_step/sec: 398.901\n",
      "INFO:tensorflow:loss = 15.129576, step = 101 (0.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 333.359\n",
      "INFO:tensorflow:loss = 12.963794, step = 201 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.944\n",
      "INFO:tensorflow:loss = 12.724754, step = 301 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 415.472\n",
      "INFO:tensorflow:loss = 11.38756, step = 401 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 381.348\n",
      "INFO:tensorflow:loss = 10.831957, step = 501 (0.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 328.046\n",
      "INFO:tensorflow:loss = 10.233763, step = 601 (0.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.82\n",
      "INFO:tensorflow:loss = 7.186685, step = 701 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 302.049\n",
      "INFO:tensorflow:loss = 11.30308, step = 801 (0.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.301\n",
      "INFO:tensorflow:loss = 8.795021, step = 901 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 404.006\n",
      "INFO:tensorflow:loss = 9.738553, step = 1001 (0.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.637\n",
      "INFO:tensorflow:loss = 13.038162, step = 1101 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.994\n",
      "INFO:tensorflow:loss = 10.713626, step = 1201 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 321.939\n",
      "INFO:tensorflow:loss = 7.2730265, step = 1301 (0.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.321\n",
      "INFO:tensorflow:loss = 9.520818, step = 1401 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 335.48\n",
      "INFO:tensorflow:loss = 11.96925, step = 1501 (0.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.52\n",
      "INFO:tensorflow:loss = 15.099467, step = 1601 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.947\n",
      "INFO:tensorflow:loss = 4.256641, step = 1701 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.022\n",
      "INFO:tensorflow:loss = 12.919135, step = 1801 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 513.059\n",
      "INFO:tensorflow:loss = 11.348013, step = 1901 (0.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.831\n",
      "INFO:tensorflow:loss = 17.77463, step = 2001 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 499.87\n",
      "INFO:tensorflow:loss = 8.710064, step = 2101 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 460.341\n",
      "INFO:tensorflow:loss = 8.424126, step = 2201 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 497.193\n",
      "INFO:tensorflow:loss = 9.678486, step = 2301 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.51\n",
      "INFO:tensorflow:loss = 13.176218, step = 2401 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 503.993\n",
      "INFO:tensorflow:loss = 9.323036, step = 2501 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 478.559\n",
      "INFO:tensorflow:loss = 9.2435255, step = 2601 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 411.102\n",
      "INFO:tensorflow:loss = 8.664817, step = 2701 (0.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.719\n",
      "INFO:tensorflow:loss = 13.132538, step = 2801 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 276.665\n",
      "INFO:tensorflow:loss = 8.547398, step = 2901 (0.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.438\n",
      "INFO:tensorflow:loss = 7.5398912, step = 3001 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 473.867\n",
      "INFO:tensorflow:loss = 11.875032, step = 3101 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 502.99\n",
      "INFO:tensorflow:loss = 11.128126, step = 3201 (0.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.44\n",
      "INFO:tensorflow:loss = 9.514978, step = 3301 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 403.791\n",
      "INFO:tensorflow:loss = 11.777414, step = 3401 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 524.134\n",
      "INFO:tensorflow:loss = 9.087822, step = 3501 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.784\n",
      "INFO:tensorflow:loss = 5.5222163, step = 3601 (0.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 381.799\n",
      "INFO:tensorflow:loss = 7.613206, step = 3701 (0.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 516.607\n",
      "INFO:tensorflow:loss = 9.498194, step = 3801 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 425.415\n",
      "INFO:tensorflow:loss = 4.269289, step = 3901 (0.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.879\n",
      "INFO:tensorflow:loss = 15.251041, step = 4001 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 429.779\n",
      "INFO:tensorflow:loss = 7.5776453, step = 4101 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 502.612\n",
      "INFO:tensorflow:loss = 15.175882, step = 4201 (0.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 284.275\n",
      "INFO:tensorflow:loss = 5.5237584, step = 4301 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 326.993\n",
      "INFO:tensorflow:loss = 7.017771, step = 4401 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 523.103\n",
      "INFO:tensorflow:loss = 11.370054, step = 4501 (0.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 497.71\n",
      "INFO:tensorflow:loss = 8.004528, step = 4601 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 494.597\n",
      "INFO:tensorflow:loss = 8.189091, step = 4701 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 516.679\n",
      "INFO:tensorflow:loss = 10.081976, step = 4801 (0.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 488.093\n",
      "INFO:tensorflow:loss = 13.196518, step = 4901 (0.204 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmplndq1w0t/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.3449707.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7f933874a860>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.train(input_fn=input_func,steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-14-04:27:51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmplndq1w0t/model.ckpt-5000\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-14-04:27:52\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.9006024, accuracy_baseline = 0.64759034, auc = 0.8943948, auc_precision_recall = 0.8608147, average_loss = 0.3330191, global_step = 5000, label/mean = 0.64759034, loss = 10.051123, prediction/mean = 0.6907083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9006024,\n",
       " 'accuracy_baseline': 0.64759034,\n",
       " 'auc': 0.8943948,\n",
       " 'auc_precision_recall': 0.8608147,\n",
       " 'average_loss': 0.3330191,\n",
       " 'global_step': 5000,\n",
       " 'label/mean': 0.64759034,\n",
       " 'loss': 10.051123,\n",
       " 'prediction/mean': 0.6907083}"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.evaluate(eval_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions:\n",
    "predictions = dnn_model.predict(pred_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmplndq1w0t/model.ckpt-5000\n"
     ]
    }
   ],
   "source": [
    "dnn_final_preds = []\n",
    "for pred in predictions:\n",
    "    dnn_final_preds.append(pred['class_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Algorithm Review:\n",
    "In the above implementation of a densely connected neural net, otherwise known as a backprop net, the model was able to succeed fairly well even with a deprived set of data. Not only did it attain significantly higher accuracy than it's perceptron predecessor, but it also succeeded in attaining higher precision and recall for both categories of classification (mouse and human). \n",
    "\n",
    "The improved performance arises due to the backprop net's enhanced ability to seperate multidimensional space. It may be the case that the deprived dataset is not quite as linearly seperable as is the full-on dataset. If this feature is true of the data, then it is clear why the DNN should have performed better. Additionally, above, the DNN was set up with a 3 layers of hidden units of the following dimensionality:\n",
    "\n",
    "`4,8,4`\n",
    "\n",
    "By adding one layer with higher dimensionality than the origional data (which had only 4 dimensions), it is possible that the algorithm was able to derive richer insights. Before arriving at this configuration of nodes, I toyed with several others, and in every case it appeared that having a hidden layer with higher dimensionality than the original data improved the performance of the model.\n",
    "\n",
    "\n",
    "To Sumarize the performance of each model with the deprived datasets, see below:\n",
    "\n",
    "### Perceptron:\n",
    "`accuracy = 0.7801205`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.39      0.56       117\n",
      "           1       0.75      0.99      0.85       215\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       332\n",
      "   macro avg       0.85      0.69      0.71       332\n",
      "weighted avg       0.82      0.78      0.75       332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN:\n",
    "`accuracy = 0.9006024` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.74      0.84       117\n",
      "           1       0.88      0.99      0.93       215\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       332\n",
      "   macro avg       0.92      0.86      0.88       332\n",
      "weighted avg       0.91      0.90      0.90       332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,dnn_final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt Classification Based on CDR and Sequence Data Alone:\n",
    "\n",
    "In this next segment of the research project, we will attempt to use only the following inputs in order to train a convolutional neural net. \n",
    "\n",
    "Although these types of nets are typically used for image processing, they can also be applied very nicely to sequence analysis. Key to their utility in this use-case is their ability to learn based off of the relationship between datapoints. Whereas basic DNNs learn from individual values, CNNs are able to learn from both the values, and their relationship to one another. \n",
    "\n",
    "## Software to be implemented:\n",
    "The software to be implemented here, [pysster](https://academic.oup.com/bioinformatics/article/34/17/3035/4962494), is a program built for the classification of biological sequences. \n",
    "\n",
    "The format required by the pysster program involves a simply formated fasta file in which the actual biological sequence is followed by a secondary structure annotation, as follows:\n",
    "- `fasta header`\n",
    "- `sequence`\n",
    "- `annotations for sequence`\n",
    "\n",
    "For example, an example sequence provided in pysster's example usage documents is as follows:\n",
    "\n",
    ">\\>chr12:96402325-96402626(+)\n",
    "UUUUUUUUUUUAAGAGUUAAGAAGCAAGAAAAAUCAGGAUUAAUGCCUUCAACAUCAAUUUUUCCCCCCAUAAAACUUAAUUUUCUAGGCUGGGCACAGUGGCUCAUGCCUGAUGCCUGUAAUUCCAGCACUUUGGGAGGCUAAGGUGGGAGGAUCACUGGAGACCAGGAGUUUGAGACCAGCCUGUACAACACAGACCCUGUUUGUAUAAAAAGUUUUAAAUUAGCCAGGCAUGGAGGCACAUGCCUGUAGUCCCAGUUACUCGGGAGGCUGAGGUGGGACAACUGACUGAGCCCAGGAG\n",
    "FFFFFFFFFFFFSSSSSSSSSIIIIIIIIIIIIIISSSSSSISSSHHHHHHHSSSISSSSSSIIIIIIIIIIIIISSSSSSSSSMMMMMSSSSSSISSSSSSSHHHHSSSMMMMMSSSSIIISSSSSSSSSSSSSSHHHHSSSSSSSSIIIIIIIIISSSSSSIISSSSSSSSSSSSSSIIIIISSSSSSSISSSSHHHSSSSSSSSSSSIIIISSSSSSSSSSMMMSSSSSSSSHHHHHHSSSSSSSSMMSSSSSSIIIISSSSSHHHHSSSSSISSSSSSMMMMMSSSSISSSSSSTTT\n",
    "\n",
    "In the above example, the first line is a name of an RNA sequence, the second line indicates the RNA sequence itself, and the thrid line indicates the structural category into which each subset of RNA fits (secondary structure of the RNA sequence).\n",
    "\n",
    "If passed two input files, formatted as indicated above, pysster will construct a convolutional neural net to attempt to decode the difference between these two inputs, based on the primary and secondary sequence information. In the example provided on pysster's page, pysster is passed three files, one in which an RNA editing location is located in an ALU repeat region, one in which it is located in a repetitive region (non-alu) and one in which it is located in a non-repetitive region. Using these three sequence files, pysster automatically randomizes the sequences, and creates a train-test-validation split. After performing these tasks, in the case of this example, pysster would perform a \"three-class single-label classification task.\"\n",
    "\n",
    "#### Application to the Project:\n",
    "In our case, we will be supplying pysster with the full antibody variable region. For our secondary structure, we will use the location of each of the antibody loops (CDRs) and frameworks. \n",
    "\n",
    "## Sequences to input:\n",
    "\n",
    "`pysster` requires all input sequences to be the same length across all files. This, however, poses a bit of a problem for us, since antibodies are inherently variable in length. Luckily the dataset we are using, the Kabat antibody dataset, includes entries of ALIGNED antibody sequences. In other words, the researchers who built this datset performed something like the sequence alignment search that we performed earlier, using clustal omega. However, in the case of these researchers, they were able to perform a more rigorous alignment...\n",
    "\n",
    "Because they knew the CDR region sequences of the antibodies in the dataset, these researchers could perform alignment simply by actually lining up CDRs with each other. Dashes were then placed in spaces where a framework residue or CDR residue *COULD* have been placed, but was in fact NOT.\n",
    "\n",
    "To visualize what this looks like, take a look at a couple of the entries in the dataset here:\n",
    "\n",
    "-EIVLTQSPGTLSLSPGERATLSCRASQS-----VSSSYLAWYQQKPGQAPRLLIYGASSRATGIPDRFSGSGSGTDFTLTISRLEPADFAVYYCQQYGSSPS-----ITFGQGTRLEI-KR-\n",
    "-QSVLTQPAS-VSGSPDQSITISCTGTSSDV---GGYKYVSWYQQHPDKAPKVMIYDVTNRPSGGSNRFSGSKSGNTASLTISGLQAEDEADYYCSSYAGAQS-----LVFGGGTKLTVLG--\n",
    "\n",
    "\n",
    "## Defining Our Alphabet\n",
    "\n",
    "In pysster, arbitrary alphabets may be defined in order to define the secondary structure of your sequence. In our case, all of our sequences follow the same pattern:\n",
    "\n",
    "`Framework 1, CDR1, Framework 2, CDR2, Framework3, CDR3, Framework 4.`\n",
    "\n",
    "\n",
    "Therefore, our arbitrarily defined alphabet will define these areas of each sequence as such. To indacte them, we will use the alphabet:\n",
    "\n",
    "`A,1,B,2,C,3,D`\n",
    "\n",
    "where letters indicate frameworks and numbers indicate CDR regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that all of the pandas, csv editor, and other forms of datapreproccessing have been removed, so as not to crowd the notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove abnormal antibodies that do not meet alignment criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humans_db = pd.read_csv('./pysster-data/human_db.csv')\n",
    "len(humans_db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humans_db = humans_db[humans_db['LightAligned'].map(len) == 123]\n",
    "len(humans_db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mouse_db = pd.read_csv('./pysster-data/mouse_db.csv')\n",
    "len(mouse_db.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mouse_db = mouse_db[mouse_db['LightAligned'].map(len) == 123]\n",
    "len(mouse_db.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an annotated antibody sequence file for use with pysster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(dataframe, fasta_out):\n",
    "    with open(fasta_out,\"w\") as output:\n",
    "        \n",
    "        # region to alphabet mapping:\n",
    "        alphabet = ['a','1','b','2','c','3','d']\n",
    "\n",
    "        for i in range(len(dataframe.index)):\n",
    "            # Fasta header:\n",
    "            \n",
    "            # Sequence (to be filled)\n",
    "            regions = []\n",
    "            annotations = ''\n",
    "            aligned_seq = dataframe.iloc[i].LightAligned\n",
    "                       \n",
    "            # Only perform annotation if we are actually looking at the data file\n",
    "            start_cdr1 = dataframe.iloc[i].Light.find(dataframe.iloc[i].L1)\n",
    "            end_cdr1 = start_cdr1 + len(dataframe.iloc[i].L1)\n",
    "            start_cdr2 = dataframe.iloc[i].Light.find(dataframe.iloc[i].L2)\n",
    "            end_cdr2 = start_cdr2 + len(dataframe.iloc[i].L2)\n",
    "            start_cdr3 = dataframe.iloc[i].Light.find(dataframe.iloc[i].L3)\n",
    "            end_cdr3 = start_cdr3 + len(dataframe.iloc[i].L3)\n",
    "            \n",
    "            # print(start_cdr1)\n",
    "            # print(end_cdr1)\n",
    "            # print(start_cdr2)\n",
    "            # print(end_cdr2)\n",
    "            # print(start_cdr3)\n",
    "            # print(end_cdr3)\n",
    "            \n",
    "            # Each of the following variables represents the number of framework and cdr residues we have\n",
    "            # In each sequence\n",
    "            f1 = (start_cdr1)\n",
    "            cdr1 = (end_cdr1 - start_cdr1)\n",
    "            f2 = (start_cdr2 - end_cdr1)\n",
    "            cdr2 = (end_cdr2 - start_cdr2)\n",
    "            f3 = (start_cdr3- end_cdr2)\n",
    "            cdr3 = (end_cdr3 - start_cdr3)\n",
    "            f4 = (len(dataframe.iloc[i].Light) - end_cdr3)\n",
    "\n",
    "            # Append these numbers to a list\n",
    "            regions = [f1,cdr1,f2,cdr2,f3,cdr3,f4]\n",
    "            \n",
    "            char_num = 0\n",
    "            # print(aligned_seq)\n",
    "            # print(regions)\n",
    "            while char_num < len(aligned_seq):\n",
    "                \n",
    "                for r in range(0,len(regions)):\n",
    "                    \n",
    "                    # Here, we decrement the value of regions[r], every time we find a residue\n",
    "                    # and put placeholder residues when we find an alignment dash\n",
    "                    # If regions[r] reaches zero, then we know we have placed all the residues in that region\n",
    "                    # print(regions[r])\n",
    "                    if regions[r] >= 0:\n",
    "                        # print(alphabet[r])\n",
    "                        annotations += alphabet[r]\n",
    "                        \n",
    "                        # If we have not found a dash, then we have found a residue\n",
    "                        if aligned_seq[char_num] != '-':\n",
    "                            regions[r] -= 1\n",
    "                    \n",
    "                        char_num += 1\n",
    "                        break\n",
    "                                                        \n",
    "            fasta_header = '>sequence' + str(i)\n",
    "            output.write(fasta_header + '\\n' + aligned_seq + '\\n' + annotations + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate(mouse_db,'./pysster-data/annotated-humans.fasta')\n",
    "annotate(humans_db, './pysster-data/annotated-mice.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training:\n",
    "Note that the following python code was heavily influcned by pysster's documentation. Pysster's reccomendations were employed throughout the code below. However, the parameters chosen here are unique to this particular project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from IPython.display import Image\n",
    "from pysster.Data import Data\n",
    "from pysster.Grid_Search import Grid_Search\n",
    "from pysster import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              class_0    class_1\n",
      "all data:         891        451\n",
      "training:         627        312\n",
      "validation:       139         62\n",
      "test:             125         77\n"
     ]
    }
   ],
   "source": [
    "# Using default random 70%/15%/15% training/validation/test split\n",
    "data = Data([\"pysster-data/annotated-humans.fasta\", \"./pysster-data/annotated-mice.fasta\"], (\"ILVFMCAGPTSYWQNHEDKR-\", \"a1b2c3d\"))\n",
    "print(data.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Parameter for a Grid Search:\n",
    "Pysster's convolutional neural net is composed of the following:\n",
    "- variable number convolutional layers (layers focused on relationship-centric data processing, essential in the proccessing of image data)\n",
    "\n",
    "- variable number max-pooling layers: Pooling takes place by slicing the input space into subregions. In the case of max pooling, the only values that the pooling layer takes as inputs are the max values in each subregion of the input space. These layers help to deemphasize the exact location of the input value, and instead place emphasis on the given input value's relationship to other input values. This serves two purposes, namely, because the number of paramaters is reduced through this practice, the compuational cost of the algorithm decreases. Secondly, because pooling results in a generalization / paramater reduction on the dataset, it helps to enhance the algorithm's capacity to generalize.\n",
    "\n",
    "- dropout layers, after the input layer and after each max-pooling layer: in these layers, some percentage of the nodes in the layer are randomly selected, each learning trial, to be completely ignored. By including drop out layers, one can avoid any one node from contributing to the learning pattern too much, and thus (hopefully) circumvent overfitting\n",
    "\n",
    "Below, we will run a 'hyperparameter grid search.' In other words, if the parameters of the convolutional neural net are things such as the number of nodes in a layer, the number of layers, etc., a grid search attempts to train a model with a variety of different user-specified parameters, and selects the model that eventually performs best.\n",
    "\n",
    "Below, we will be searching for the best combination of the following variables:\n",
    "- number of convolutional layers (1 or 2)\n",
    "- number of kernels in each layer (20 or 60)\n",
    "- percentage of nodes to drop out in dropout layers (10% or 40%)\n",
    "\n",
    "In total, by combining each of these different options, we will be training 8 different models.\n",
    "\n",
    "Features we won't attempt to optimize through the grid search:\n",
    "- length of these kernels (20)\n",
    "*CNNs employ nodes in three dimensional space, thus they take both number and length for a number of kernels*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"conv_num\": [1, 2], \"kernel_num\": [20,60], \"kernel_len\": [20], \"dropout_input\": [0.1, 0.4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = Grid_Search(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time in minutes: 0.8162944475809734\n"
     ]
    }
   ],
   "source": [
    "model, summary = searcher.train(data, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conv_num: [1, 2]\n",
      "# kernel_num: [20, 60]\n",
      "# kernel_len: [20]\n",
      "# dropout_input: [0.1, 0.4]\n",
      "conv_num kernel_num kernel_len dropout_input roc-auc\n",
      "       1         20         20           0.4 0.98805\n",
      "       1         60         20           0.4 0.98561\n",
      "       1         20         20           0.1 0.97946\n",
      "       2         20         20           0.1 0.97459\n",
      "       1         60         20           0.1 0.97447\n",
      "       2         20         20           0.4 0.97424\n",
      "       2         60         20           0.4 0.97122\n",
      "       2         60         20           0.1 0.95753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   roc-auc    pr-auc          n\n",
      "     class_0     0.953     0.976     0.964     0.992     0.995  |     125\n",
      "     class_1     0.959     0.922     0.940     0.992     0.995  |      77\n",
      "\n",
      "\n",
      "weighted avg     0.956     0.955     0.955     0.992     0.995  |        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_folder = 'performance_images/'\n",
    "predictions = model.predict(data, \"test\")\n",
    "labels = data.get_labels(\"test\")\n",
    "utils.plot_roc(labels, predictions, out_folder+\"roc.png\")\n",
    "utils.plot_prec_recall(labels, predictions, out_folder+\"prec.png\")\n",
    "print(utils.get_performance_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(out_folder+\"roc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(out_folder+\"prec.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating The Model:\n",
    "As we can see, our model was actually able to perform very well using only seqeunce data. We can attibute this to a variety of factors, namely:\n",
    "\n",
    "- The power of the NVIDIA GPU we used to compute this model enabled a complex CNN and many training trials\n",
    "\n",
    "- Framework regions are conserved fairly well across most sequences generated by a given species. They experience very little genetic variablity. Therefore, the algorithm likely was able to pick up on framework region motifs, and classify species this way.\n",
    "\n",
    "- To truly test the models power, one would have to feed it only CDR region sequences, subject to significantly higher genetic variablity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
